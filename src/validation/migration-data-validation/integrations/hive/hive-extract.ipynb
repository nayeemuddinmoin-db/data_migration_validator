{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec1635b2-0f81-4eb3-b627-e76f446f8397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "#  TODO                                  #\n",
    "#  @author: Mahesh Madhusoodanan Pillai  #\n",
    "#  @email: mahesh.pillai@databricks.com  #\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "612e2eef-4115-4af8-855a-1665aec72223",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from py4j.java_gateway import java_import\n",
    "gw = spark.sparkContext._gateway\n",
    "java_import(gw.jvm, \"dbsqlDialectClass\")\n",
    "gw.jvm.org.apache.spark.sql.jdbc.JdbcDialects.registerDialect(\n",
    "  gw.jvm.dbsqlDialectClass())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ab311ee-283c-40c6-b7e4-ca0fef55decc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def runSimpleHiveSQL(sql, jdbc_options):\n",
    "  print(jdbc_options)\n",
    "  # jdbc_options = json.loads(jdbc_options)\n",
    "  hostname = jdbc_options[\"hostname\"]\n",
    "  port = jdbc_options[\"port\"]\n",
    "  auth = jdbc_options[\"auth\"]\n",
    "  conn = hive.connect(host=hostname, port=port, username='')\n",
    "  \n",
    "  if not(auth.lower() == 'none'):\n",
    "    user = dbutils.secrets.get(\n",
    "        scope=jdbc_options[\"user\"][\"secret_scope\"], key=jdbc_options[\"user\"][\"key\"]\n",
    "    )\n",
    "    password = dbutils.secrets.get(\n",
    "        scope=jdbc_options[\"password\"][\"secret_scope\"],\n",
    "        key=jdbc_options[\"password\"][\"key\"],\n",
    "    )\n",
    "    # password can be used only if the auth with auth='LDAP' or auth='CUSTOM' only\n",
    "    conn = hive.connect(host=hostname, port=port, auth = auth, username='', password=password)\n",
    "\n",
    "  cursor = conn.cursor()\n",
    "  # query = f'DESC {table}'\n",
    "  # print(f\"Capturing Hive Schema for the table: {table}\")\n",
    "  cursor.execute(sql)\n",
    "  results = cursor.fetchall()\n",
    "  df = pd.DataFrame(results, columns=[col[0] for col in cursor.description])\n",
    "  cursor.close()\n",
    "  conn.close()\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35f87c4a-4d82-4d5a-857a-b8960646a585",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, to_timestamp, coalesce\n",
    "from pyhive import hive\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def captureHiveSchema(table, jdbc_options):\n",
    "    print(table)\n",
    "    splits = table.split(\".\")\n",
    "    db_name, tbl_name = splits\n",
    "\n",
    "   \n",
    "    query = f'DESC {table}'\n",
    "    print(f\"Capturing Hive Schema for the table: {table}\")\n",
    "    df = runSimpleHiveSQL(query, jdbc_options)\n",
    "    tbl_schema = spark.createDataFrame(df)\n",
    "    # spark_df.display()\n",
    "\n",
    "    print(f\"{table}: {tbl_schema.show()}\")\n",
    "    tbl_schema.createOrReplaceTempView(f\"tbl_schema_{tbl_name}\")\n",
    "    tbl_schema_with_row_num = spark.sql(\n",
    "        f\"\"\"select '{db_name}' as db_name, '{tbl_name}' as table_name, original_order, col_name, data_type, comment from (SELECT\n",
    "            *,\n",
    "            ROW_NUMBER() OVER (\n",
    "                PARTITION BY col_name\n",
    "                ORDER BY\n",
    "                original_order\n",
    "            ) AS rn\n",
    "            FROM\n",
    "            (\n",
    "                select\n",
    "                row_number() over (\n",
    "                    order by\n",
    "                    a\n",
    "                ) as original_order,\n",
    "                col_name as col_name,\n",
    "                data_type as data_type,\n",
    "                `comment` as\n",
    "                comment\n",
    "                from\n",
    "                (\n",
    "                    select\n",
    "                    monotonically_increasing_id() a,\n",
    "                    *\n",
    "                    from\n",
    "                    tbl_schema_{tbl_name}\n",
    "                )\n",
    "                where\n",
    "                -- to address partitioning and clustering columns\n",
    "                data_type not in ('data_type', '')\n",
    "            ))x where rn = 1\"\"\"\n",
    "    )\n",
    "    tbl_schema_with_row_num.show()\n",
    "    \n",
    "    return tbl_schema_with_row_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6030a79-1f80-4f24-bc40-f5f4aaa52535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def processHiveColNames(table, col_mapping, mismatch_exclude_fields, jdbc_options):\n",
    "  cm = col_mapping\n",
    "  db_name = table.split(\".\")[0]\n",
    "  # mismatch_exclude_fields_compiled = mismatch_exclude_fields_string.format(**locals())\n",
    "  # mismatch_exclude_fields = [field.strip() for field in mismatch_exclude_fields_compiled.split(\"|\")]  \n",
    "  \n",
    "\n",
    "  # column_sql = f\"SHOW COLUMNS FROM {table}\"\n",
    "  column_sql = f\"(SELECT * FROM {table} where 1=0)a\"\n",
    "  # columns = runSimpleHiveSQL(column_sql, jdbc_options_json)\n",
    "  columns = readHive(column_sql, db_name, jdbc_options, additional_options={}).columns\n",
    "  print(f\"insideHive {mismatch_exclude_fields}\")\n",
    "  # Remove entries against mismatch_exclude_fields\n",
    "  columns = [col for col in columns if col not in mismatch_exclude_fields]\n",
    "  print(columns)\n",
    "  col_dict = {}\n",
    "  # replace the column names with the mapped column names from the user\n",
    "  col_dict = {col: cm.get(col, col) for col in columns}\n",
    "  # Sorting dictionary by values (ascending order)\n",
    "\n",
    "  sorted_col = dict(sorted(col_dict.items(), key=lambda item: item[1]))\n",
    "  col_cast_list = \", \".join([f\"COALESCE(CAST({key} AS STRING),'') as {key}\" for key in sorted_col.keys()])\n",
    "  col_list = \", \".join(sorted_col.keys())\n",
    "  return col_list, col_cast_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6983bb01-34aa-4156-a9d6-4e88e02bff4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generateHiveSqls(table, primary_keys_string, table_mapping, jdbc_options, sql_override, data_load_filter):\n",
    "  # pk_columns = table_mapping.tgt_primary_keys\n",
    "  # primary_keys_string = pk_columns.replace(\"|\", \",\")\n",
    "  # print (primary_keys_string)\n",
    "  quick_validation = table_mapping.quick_validation\n",
    "  col_mapping = table_mapping.col_mapping\n",
    "  mismatch_exclude_fields_string = table_mapping.mismatch_exclude_fields\n",
    "\n",
    "  # quick_validation = True\n",
    "\n",
    "\n",
    "  \n",
    "  load_filter = data_load_filter if (not data_load_filter is None) else \"1=1\"\n",
    "  read_sql = sql_override if (not sql_override is None) else f\"select * from {table}\"\n",
    "\n",
    "  # read_sql_compiled = f\"({read_sql.format(**locals())})a\"\n",
    "  read_sql_compiled = f\"(SELECT hash({primary_keys_string}) as pk_hash_mmp, a.* FROM  ({read_sql.format(**locals())})a where {load_filter})b\"\n",
    "  boundary_sql = f\"(SELECT min(hash({primary_keys_string})) lower_bound, max(hash({primary_keys_string})) upper_bound  FROM {read_sql_compiled})c\"\n",
    "\n",
    "  # if quick_validation:\n",
    "  #   processColNames(table, col_mapping, primary_keys_string, mismatch_exclude_fields_string, jdbc_options)\n",
    "\n",
    "   \n",
    "  print(f\"{read_sql}\\n{read_sql_compiled}\")\n",
    "  return boundary_sql, read_sql_compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a71b8a9b-3abd-4b5b-a4c7-f1901e2918e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def readHive(query, db_name, jdbc_options, additional_options={}):\n",
    "    # jdbc_options = json.loads(jdbc_options_json)\n",
    "    hostname = jdbc_options[\"hostname\"]\n",
    "    port = jdbc_options[\"port\"]\n",
    "    auth = jdbc_options[\"auth\"]\n",
    "\n",
    "    url = f\"jdbc:hive2://{hostname}:{port}/{db_name};hive.exec.dynamic.partition.mode=nonstrict;hive.tez.container.size=8192;hive.tez.java.opts=-Xmx6g;tez.runtime.io.sort.mb=4096;tez.runtime.unordered.output.buffer.size-mb=1024;hive.exec.max.dynamic.partitions=10000;hive.exec.max.dynamic.partitions.pernode=500;\"\n",
    "    \n",
    "    user = \"\" if (auth.lower() == 'none') else dbutils.secrets.get(\n",
    "        scope=jdbc_options[\"user\"][\"secret_scope\"], key=jdbc_options[\"user\"][\"key\"]\n",
    "    )\n",
    "    password = \"\" if (auth.lower() == 'none') else dbutils.secrets.get(\n",
    "        scope=jdbc_options[\"password\"][\"secret_scope\"],\n",
    "        key=jdbc_options[\"password\"][\"key\"],\n",
    "    )\n",
    "\n",
    "    reader_options = {\n",
    "        \"url\": url,\n",
    "        \"auth\": auth,\n",
    "        \"user\": user,\n",
    "        \"password\": password,\n",
    "        # \"fetchsize\": \"10000\",\n",
    "        # \"compression\": \"snappy\",\n",
    "    }\n",
    "\n",
    "    final_reader_options = {**reader_options, **additional_options}\n",
    "    print(f'jdbc_query:{query}')\n",
    "    df = (\n",
    "        spark.read.format(\"jdbc\")\n",
    "        # .option(\"driver\", \"org.apache.hive.jdbc.HiveDriver\") #default\n",
    "        .option(\"driver\", \"com.amazon.hive.jdbc.HS2Driver\") #EMR\n",
    "        # .option(\"driver\", \"org.apache.hive.jdbc.HiveDriver\") #FromEMRCluster\n",
    "        # .option(\"driver\", \"com.cloudera.hive.jdbc4.HS2Driver\")  #Cloudera\n",
    "        .option(\"dbtable\", query)\n",
    "        .options(**final_reader_options)\n",
    "        .load())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e036d105-a3f7-4c2d-bb90-5ddd42170c00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def captureHiveTableHash(table, primary_keys_string, mismatch_exclude_fields, sql_override, data_load_filter, table_mapping, jdbc_options):\n",
    "\n",
    "  db_name = table.split(\".\")[0]\n",
    "  # pk_columns = table_mapping[\"tgt_primary_keys\"]\n",
    "  # primary_keys_string = pk_columns.replace(\"|\", \",\")\n",
    "  print (primary_keys_string)\n",
    "  col_mapping = table_mapping.col_mapping\n",
    "  # mismatch_exclude_fields_string = table_mapping.mismatch_exclude_fields\n",
    "\n",
    "  load_filter = data_load_filter if (not data_load_filter is None) else \"1=1\"\n",
    "  read_sql = sql_override if (not sql_override is None) else f\"select * from {table}\"\n",
    "\n",
    "  # read_sql_compiled = f\"({read_sql.format(**locals())})a\"\n",
    "  read_sql_compiled = f\"(SELECT a.* FROM ({read_sql.format(**locals())})a where {load_filter})b\"\n",
    "\n",
    "  col_list, col_cast_list = processHiveColNames(read_sql_compiled, col_mapping, mismatch_exclude_fields, jdbc_options)\n",
    "\n",
    "  #one with concatenated values for debugging\n",
    "  # sql = f\"\"\"(SELECT concat_ws(\":\",{primary_keys_string}) as p_keys, sha2(concat_ws(\":\",{col_list}),256) as row_hash, concat_ws(\":\",{col_list}) as val from (SELECT {col_cast_list} from {read_sql_compiled})a)b\"\"\"\n",
    "\n",
    "  sql = f\"\"\"(SELECT concat_ws(\":\",{primary_keys_string}) as p_keys, sha2(concat_ws(\":\",{col_list}),256) as row_hash from (SELECT {col_cast_list} from {read_sql_compiled})a)b\"\"\"\n",
    "  print(sql)\n",
    "\n",
    "  df = readHive(sql, db_name, jdbc_options)\n",
    "  df_renamed = df.select([col(f\"`{c}`\").alias(re.sub('^b\\.', '', c)) for c in df.columns])\n",
    "\n",
    "  df_renamed.show()\n",
    "  return df_renamed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e873fca7-d4f0-410d-b6c2-dfe6a89ec84b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType    \n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def captureHiveTable(table, primary_keys_string, sql_override, data_load_filter, table_mapping, jdbc_options):\n",
    "\n",
    "    db_name = table.split(\".\")[0]\n",
    "\n",
    "    # jdbc_options = json.loads(jdbc_options_json)\n",
    "    src_cast_to_string = table_mapping.src_cast_to_string\n",
    "\n",
    "\n",
    "    boundary_sql, read_sql_compiled = generateHiveSqls(table, primary_keys_string, table_mapping, jdbc_options, sql_override, data_load_filter)\n",
    "    print (f\"Retrieveing the boundaries\")\n",
    "\n",
    "    df_boundary = readHive(boundary_sql, db_name, jdbc_options).collect()[0]\n",
    "\n",
    "    lowerbound, upperbound = df_boundary[\"c.lower_bound\"], df_boundary[\"c.upper_bound\"]\n",
    "    print(f\"Capturing Hive Contents for the table: {table}\")\n",
    "\n",
    "    #for cases where the hash validation returns no anomalies resulting in no data. To prevent null poitnter exception from the partition conditions.\n",
    "    print(f'{lowerbound},{upperbound}')\n",
    "    if (not(lowerbound is None) and not(upperbound is None)):\n",
    "        partition_options ={\n",
    "            \"partitionColumn\":\"b.pk_hash_mmp\",\n",
    "            \"numPartitions\":5,\n",
    "            \"lowerBound\":lowerbound,\n",
    "            \"upperBound\":upperbound,\n",
    "            \"fetchSize\":1000000\n",
    "            }\n",
    "        final_reader_options = {**partition_options, **jdbc_options[\"additional_options\"]}\n",
    "    else:\n",
    "        final_reader_options = jdbc_options[\"additional_options\"]\n",
    "\n",
    "    df = readHive(read_sql_compiled, db_name, jdbc_options, final_reader_options)\n",
    "    \n",
    "    # Remove the \"b.\" prefix from the column names \n",
    "    df_renamed = df.select([col(f\"`{c}`\").alias(re.sub('^b\\.', '', c)) for c in df.columns])\n",
    "    # Drop the jdbc partition column\n",
    "    df_renamed_dropped = df_renamed.drop(\"pk_hash_mmp\")\n",
    "\n",
    "    # df = spark.read.table(table).filter(load_filter)\n",
    "    to_str = df_renamed_dropped.columns\n",
    "    if src_cast_to_string:\n",
    "    #Convert all fields to String\n",
    "        for cols in to_str:\n",
    "            df_renamed_dropped = df_renamed_dropped.select([df_renamed_dropped[c].cast(StringType()).alias(c) for c in to_str])\n",
    "    return df_renamed_dropped"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "hive-extract",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
